 
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic faces with specific attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this blog post is to...no one has done this ...not tutorial on gans...the complete code can be found here, here is just a summary\n",
    "For this purpose, we are going to use Generative Adversarial Networks (GANs). They are generative neural network models which aim to produce images that look like real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CelebA data set (link here) which is composed of roughly 200k images. We will use 180k of these for training our generative model and 20k for evaluate the model accordind to the FID score (lin here).\n",
    "The training images has been pre-processed such that have a fixed size of $64 \\times 64$ and cropped in such a way to have the faces centered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are intrinsically difficult to train due their tendency to diverge. So, before diving into the code, I am going to super briefly introduce three types of layers I added in the GAN to obtain better results:\n",
    "- **Pixel normalization**: is a normalization technique whose aim is to control the magnitude of the activations of the generator. It normalize the feature vector in each pixel to unit vector after each convolutional layer in the generator. This layer helps to obtain better images.\n",
    "- **Mini-Batch Standard Deviation**: this layer is used to increase the diversity of the generated images.\n",
    "- **Spectral Normalization**: is a weight normalization that stabilizes the training of the discriminator in order to limitate the exploding gradient problem and the mode collapse problem. This layer helps the GAN to converge and have a smoother loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a deep learning architecture for training powerful generator models. A generator model is capable of generating new artificial samples that plausibly could have come from an existing distribution of samples. GANs are comprised of both generator and discriminator models. The generator is responsible for generating new samples from the domain (real faces in our case), and the discriminator is responsible for classifying whether samples are real or fake. Importantly, the performance of the discriminator model is used to update both the model weights of the discriminator itself and the generator model. This means that the generator never actually sees examples from the domain and is adapted based on how well the discriminator performs.\n",
    "\n",
    "However we need a way to condition the generation of the generated images. For this purpose, we are gointo to make use of the Auxiliary Classifier GAN (ACGAN) which is an extension of the GAN architecture that are able to achieve our task.\n",
    "The ACGAN's discriminator is provided with the image as input and it must predict whether the given image is real or fake and must also predict the attributes vector of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am going to show the code for the definition of the network of the `discriminator`, `generator` and the `acgan` combined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(n_attr, filters = 128, kernel_size = 3, in_shape=(64,64,3)):\n",
    "\n",
    "    in_img = Input(shape=(in_shape))\n",
    "    \n",
    "    # 64 x 64 x FILTERS\n",
    "    fe = SNConv2D(filters=filters, kernel_size=kernel_size, strides=(2,2), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(in_img)\n",
    "    fe = LeakyReLU(alpha=0.1)(fe)\n",
    "    fe = MinibatchStdev()(fe)\n",
    "    \n",
    "    # 32 x 32 x FILTERS\n",
    "    fe = SNConv2D(filters=filters * 2, kernel_size=kernel_size, strides=(2,2), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(fe)\n",
    "    fe = LeakyReLU(alpha=0.1)(fe)\n",
    "   \n",
    "    # 16 x 16 x FILTERS\n",
    "    fe = SNConv2D(filters=filters * 4, kernel_size=kernel_size, strides=(2,2), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(fe)\n",
    "    fe = LeakyReLU(alpha=0.1)(fe)\n",
    "   \n",
    "    # 8 x 8 x FILTERS\n",
    "    fe = SNConv2D(filters=filters * 8, kernel_size=kernel_size, strides=(2,2), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(fe)\n",
    "    fe = LeakyReLU(alpha=0.1)(fe)\n",
    "    \n",
    "    # current: 4 x 4 x FILTERS\n",
    "    fe = GlobalAveragePooling2D()(fe)\n",
    "    \n",
    "    # output about fake/real image:\n",
    "    out1 = Dense(1, activation='sigmoid')(fe)\n",
    "    \n",
    "    # output about attributes\n",
    "    out2 = Dense(n_attr, activation='sigmoid')(fe)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(in_img, [out1, out2])\n",
    "    \n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"], optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(latent_dim, n_attr, filters=128, kernel_size=4):\n",
    "\n",
    "    in_gen = Input(shape=(latent_dim + n_attr,))\n",
    "    gen = Dense(4 * 4 * filters * 8)(in_gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((4, 4, filters * 8))(gen)\n",
    "    gen = PixelNormalization()(gen)\n",
    "    \n",
    "    # 4x4 -> 8x8\n",
    "    gen = UpSampling2D()(gen)\n",
    "    gen = SNConv2D(filters=filters * 4, kernel_size=kernel_size, strides=(1,1), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(gen)\n",
    "    gen = PixelNormalization()(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    # 8x8 -> 16x16\n",
    "    gen = UpSampling2D()(gen)\n",
    "    gen = SNConv2D(filters=filters * 2, kernel_size=kernel_size, strides=(1,1), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(gen)\n",
    "    gen = PixelNormalization()(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    # 16x16 -> 32x32\n",
    "    gen = UpSampling2D()(gen)\n",
    "    gen = SNConv2D(filters=filters, kernel_size=kernel_size, strides=(1,1), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(gen)\n",
    "    gen = PixelNormalization()(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    # 32x32 -> 64x64\n",
    "    gen = UpSampling2D()(gen)\n",
    "    gen = SNConv2D(filters=filters // 2, kernel_size=kernel_size, strides=(1,1), padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(gen)\n",
    "    gen = PixelNormalization()(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    # 64 x 64 x FILTERS -> 64 x 64 x 3\n",
    "    image = SNConv2D(filters=3, kernel_size=4, strides=(1, 1), activation='tanh', padding='same', kernel_initializer=\"orthogonal\", spectral_normalization=True)(gen)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(in_gen, image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACGAN combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def acgan(g_model, d_model):\n",
    "    img = g_model.output\n",
    "    d_model.trainable = False\n",
    "    valid, target_label = d_model(img)\n",
    "    \n",
    "    noise_plus_label = g_model.input\n",
    "    model = Model(noise_plus_label, [valid, target_label])\n",
    "\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to train the model is the onw below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, batch_size):\n",
    "    bat_per_epo = int(dataset[0].shape[0] / batch_size)\n",
    "    half_batch = int(batch_size / 2)\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)   \n",
    "            d_metrics1 = d_model.train_on_batch(X_real, [y_real, labels_real])\n",
    "            d_loss1 = d_metrics1[0]\n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_metrics2 = d_model.train_on_batch(X_fake, [y_fake, labels])\n",
    "            d_loss2 = d_metrics2[0]\n",
    "            [z_input, z_labels] = generate_latent_points(latent_dim, batch_size, n_attr, n_classes=2)\n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            concat = tf.concat([z_input, z_labels], axis=-1)\n",
    "            g_metrics = gan_model.train_on_batch(concat, [y_gan, z_labels])\n",
    "            g_loss = g_metrics[0]\n",
    "            \n",
    "        # summarize loss on this batch\n",
    "        print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
    "        \n",
    "        # save the generator model\n",
    "        g_model.save('cgan_generator' +str(i+j)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 180k training images as we said before and we are going to use all the CelebA attributes which are 40. I used 16 as batch size becuase this was the largest value I could fit on my GPU, 128 as latent space and I trained for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of images to use for training\n",
    "how_many = 180000\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 128\n",
    "\n",
    "# number of images per batch\n",
    "batch_size = 16\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# number of attributes per image\n",
    "n_attr = 40\n",
    "\n",
    "# just 2 possible value for each attribute\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the discriminator\n",
    "d_model = discriminator(n_attr)\n",
    "\n",
    "# create the generator\n",
    "g_model = generator(latent_dim, n_attr)\n",
    "\n",
    "# create the gan combined model\n",
    "gan_model = acgan(g_model, d_model)\n",
    "\n",
    "# load dataset\n",
    "dataset = load_dataset(how_many)\n",
    "\n",
    "# train model\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, batch_size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some results aftr roughly 48 hours of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
