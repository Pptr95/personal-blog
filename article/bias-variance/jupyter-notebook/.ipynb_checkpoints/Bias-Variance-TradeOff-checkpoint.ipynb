{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias - Variance tradeoff and Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the following article is to explore the relationship between two competing properties of a statistical learning model: **bias** and **variace**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important concept in machine learning is the **bias-variance** tradeoff. Model with high bias are not complex enough for the data and then to underfit, whilst models with high variance overfit to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *variance* consists in the amount by which a model changes when we train it on a different data set. It is expected that the fitted method varies according to the data it is fed with, nevertheless the change shouldn't be significant. If this is not the case the model is generally too flexible and is said to have *high variance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the *bias* is the error introduced when we try to model a real world problem, which may be very complicated, with a too simple approximation. \n",
    "\n",
    "So the **goal** is to create a model which can find a **tradeoff** between *bias* and *variace*.\n",
    "\n",
    "To better understand the problem we will do an example. In this example we try to predict the water flowing out of a dam using the change in water level of an external reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, the more we increase the flexibity of a method, the more the variance increases and the bias decreases. The rate at which the two properties change is not the same though and it is crucial to study their relative fluctuation to find the sweet spot minimizing the total model error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The water and the dam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As announced above we will build a predictive model to forecast the amount of water getting out of a dam. We will do this playing around with Linear Regression (checkout my [article](http://petrupotrimba.altervista.org/image-post.php?id=1#img) if you are not familiar with Linear Regression) at first drawing a straight line and later on extending the feature space with the addition of p-th degree polynomials of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First recall the regularized cost function and its gradient for Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} $ for $j=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \\frac{\\lambda}{m}\\sum_{j=1}^n\\theta_j$ for $j\\geq1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy import optimize\n",
    "plt.rcParams['figure.figsize'] = (18, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('ex5data1.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "X_val = mat['Xval']\n",
    "y_val = mat['yval']\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "ax.scatter(X, y, s=500, marker='x',color='r',linewidths=8)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can se seen that the relation between X and y is not linear. Nevertheless, for the purpose of experimenting with ML we will fit a linear model and check the results we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below there are some useful functions I created that we will use throughout this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(theta, X, y, lamda):\n",
    "    m = y.shape[0]\n",
    "    h_y = X.dot(theta) - y\n",
    "    cost = (1/(2*m)) * (sum(h_y**2)) + (lamda/(2*m))*(sum(theta[1:]**2))\n",
    "    return cost\n",
    "\n",
    "def computeGradientReg(theta, X, y, lamda):\n",
    "    theta = theta.reshape(-1, 1)\n",
    "    m = y.shape[0]\n",
    "    h_y = X.dot(theta) - y\n",
    "    grad = 1/m * (h_y).T.dot(X)\n",
    "    grad[1:] = grad + lamda/m*theta[1:].T\n",
    "    return grad\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, num_iters, lamda):\n",
    "    m = y.shape[0]\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    for i in range(num_iters):\n",
    "        h_y = X.dot(theta) - y\n",
    "        gradient = (alpha/m)*((X.T).dot(h_y))\n",
    "        gradient[1:] = gradient[1:] + (lamda/m*theta[1:])\n",
    "        theta = theta - gradient\n",
    "        J_history[i] = costFunctionReg(theta, X, y, lamda)\n",
    "       \n",
    "    return theta, J_history\n",
    "\n",
    "def learningCurve(X, y, Xval, yval, lamda):\n",
    "    m =  X.shape[0]\n",
    "    initial_theta = np.ones((X.shape[1], 1))\n",
    "    error_train = np.zeros((m, 1))\n",
    "    error_val = np.zeros((m, 1))\n",
    "    for i in range(m):\n",
    "        theta = gradientDescent(X[0:i+1], y[0:i+1], initial_theta, 0.001, 3000, lamda)[0]\n",
    "        error_train[i] = costFunctionReg(theta, X[0:i+1], y[0:i+1], lamda)\n",
    "        error_val[i] = costFunctionReg(theta, Xval, yval, lamda)\n",
    "    return error_train, error_val\n",
    "\n",
    "def polyFeatures(X, p):\n",
    "    for i in np.arange(1, p + 1):\n",
    "        X = np.c_[X, X[:, 0]**i]\n",
    "    return X\n",
    "\n",
    "def featureNormalize(X):\n",
    "    average = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - average)/sigma\n",
    "    return X_norm, average, sigma\n",
    "\n",
    "def plotFit(min_X, max_X, mu, sigma, p):\n",
    "    X = np.arange(min_X - 15, max_X + 15, 0.05).reshape(-1,1)\n",
    "    X_poly = polyFeatures(X, p)\n",
    "    X_poly = (X_poly - mu) / sigma\n",
    "    X_poly = np.c_[np.ones(X_poly.shape[0]), X_poly]\n",
    "    return X, X_poly\n",
    "\n",
    "def validationCurve(X, y, Xval, yval, lambda_vec):\n",
    "    m = lambda_vec.size\n",
    "    error_train = np.zeros((m, 1))\n",
    "    error_val = np.zeros((m, 1))\n",
    "    initial_theta = np.zeros((X.shape[1], 1))\n",
    "    alpha = 0.001\n",
    "    num_iters = 3000\n",
    "    for i, lamda in enumerate(lambda_vec):\n",
    "        theta = gradientDescent(X, y, initial_theta, alpha, num_iters, lamda)[0]\n",
    "        error_train[i] = costFunctionReg(theta, X, y, 0)\n",
    "        error_val[i] = costFunctionReg(theta, Xval, yval, 0)\n",
    "    best_lamda = lambda_vec[error_val.argmin()]\n",
    "    return error_train, error_val, best_lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Bias model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run Linear Regression. To minimaze Linear Regression cost function we use *gradient descent* algorithm which we have just implemented above. We set our initial parameters `initial_theta` to one and the regualarization parameter `lamda` to zero since in this section we do not want to regularize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: intercept=[12.42957875], slope=[0.36383098]\n"
     ]
    }
   ],
   "source": [
    "# add bias term\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0\n",
    "\n",
    "initial_theta = np.ones((X.shape[1], 1))\n",
    "num_iters = 3000\n",
    "alpha = 0.001\n",
    "theta, J_history = gradientDescent(X, y, initial_theta, alpha, num_iters, lamda)\n",
    "print(\"Theta found by gradient descent: intercept={0}, slope={1}\".format(theta[0],theta[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that *gradient descent* have worked properly, we plot the cost value of the cost function to make sure it non increase with each iteration of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(range(num_iters), J_history)\n",
    "plt.ylabel('Cost Value')\n",
    "plt.xlabel('Iteration of Gradient Descent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is decreasing with each iteration, good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the moment to plot the fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "ax.plot(X[:, 1], y[:, 0], 'x', label='Training Data')\n",
    "ax.plot(X[:,1], np.matmul(X, theta), linestyle='-', label='Linear Regression')\n",
    "# set the legend for the labels: 'Training Data' and 'Linear Regression'\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.ylabel('Change in water level (x)')\n",
    "plt.xlabel('Water flowing out of the dam (y)')#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that a straight line does not model the true relationship between X and y. The method is not flexible enough, resulting in **high bias** and **low variance**. The finding is proved by the below learning curve as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **learning curve** is a useful method that shows the error on the training set v.s. validation set for an increasing number of data points ingested in the learning process. Let's call `learningCurve` function and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias term validation set\n",
    "X_val = np.c_[np.ones((X_val.shape[0], 1)), X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "error_train, error_val = learningCurve(X, y, X_val, y_val, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(range(X.shape[0]), error_train, label = 'Training set error')\n",
    "ax.plot(range(X.shape[0]), error_val, label = 'Validation set error')\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Training samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that *both* the train error and cross validation error are high when the number of training examples is increased. As we already discovered before looking at the line that fits the training set, this reflects a **high bias** problem in the model, namely the linear regression model is too simple and is unable to fit our dataset well. To fit a better model we will implement **polynomial regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our linear model was that it was too simple for the data and resulted **underfitting** (high bias). In this part, we will address this problem by adding more features and our hypothesis will have the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_\\theta(x) = \\theta_0 + \\theta_1 * (waterLevel) + \\theta_2 * (waterLevel)^2 + \\dots + \\theta_p * (waterLevel)^p =\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_px_p$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, by doing that mapping, we obtain a linear regression model where the features are the various powers of the original value (waterLevel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve that, I implemented the function `polyFeatures` which takes in input the dataset we want to add polynomial features and the max polynomial degree.\n",
    "It turns out that if we run the training directly using the new dataset we obtained, will not work well as the features would be badly scaled. Therefore is necessary to use feature normalization. To do that, I implemented the function `featureNormalize`. This function returns the normalized dataset, the *mean* and the *standard deviation* (represented by `sigma` variable in the code). We need the mean and standard deviation for normalize subsequently the validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Variance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('ex5data1.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "X_val = mat['Xval']\n",
    "y_val = mat['yval']\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10\n",
    "\n",
    "X_poly = polyFeatures(X, p)\n",
    "# it is important to normalize and scale our dataset before applying any ML on it\n",
    "X_poly, mean, sigma = featureNormalize(X_poly)\n",
    "# add biad term to our new dataset\n",
    "X_poly = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]\n",
    "\n",
    "X_poly_val = polyFeatures(X_val, p)\n",
    "X_poly_val = (X_poly_val - mean) / sigma\n",
    "X_poly_val = np.c_[np.ones((X_poly_val.shape[0], 1)), X_poly_val]\n",
    "\n",
    "X_poly_test = polyFeatures(X_test, p)\n",
    "X_poly_test = (X_poly_test - mean) / sigma\n",
    "X_poly_test = np.c_[np.ones((X_poly_test.shape[0], 1)), X_poly_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized our datasets, we can run Linear Regression on the polynomial dataset and see how good our line fits the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "alpha = 0.001\n",
    "num_iters = 3000\n",
    "initial_theta = np.ones((X_poly.shape[1], 1))\n",
    "theta_fit_poly, J_history_poly = gradientDescent(X_poly, y, initial_theta, alpha, num_iters, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "ax.plot(X, y, 'x', color='r')\n",
    "X_fit, X_poly_fit = plotFit(min(X), max(X), mean, sigma, p)\n",
    "ax.plot(X_fit, np.dot(X_poly_fit, theta_fit_poly), '--', color='blue')\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our linear model was that it was too simple for the data and resulted in underfitting (high bias).\n",
    "The polynomial fit, on the contrary, is able to follow the datapoints very well - thus, obtaining a low training error. However, the polynomial method is very complex and even drops off at the extremes. This is an indicator that the polynomial regression model is **overfitting** the training data and will not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the problems with the unregularized model ($\\lambda = 0$), you can see that the learning curve plot blow shows the same effect where the training error is low, but the validation error is high. There is a gap between the training and cross validation errors, indicating a **high variance** problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "error_train_poly, error_val_poly = learningCurve(X_poly, y, X_poly_val, y_val, lamda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(range(X_poly.shape[0]), error_train_poly, label = 'Training set error')\n",
    "ax.plot(range(X_poly.shape[0]), error_val_poly, label = 'Validation set error')\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Training samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remedy the situation we have to reduce the variance. **Regularization** come to the rescue; specifically now we are going to apply the *validation set approach* to select the *best* value of lambda that minimize the validation set error. <br>\n",
    "Minimizing that validation set error means we created a model with no bias and no variance, which is our goal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularize the model selecting $\\lambda$ using a cross validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ is a regularization parameter which controls the degree of regularization (thus, helps preventing overfitting). The regularization term puts a penalty on the overall cost $J$. As the magnitudes of the model parameters $\\theta_j$ increase, the penalty increases as well.\n",
    "\n",
    "The regularization parameter $\\lambda$ can significantly affect the results of regularized polynomial regression on the training and cross validation set. In particular, a model without regularization ($\\lambda = 0$) fits the trainig set well, but does not generalize. Conversely, a model with too much regularization ($\\lambda = 100$) does not fit the training set and testing set well. A good choice of $\\lambda$ (e.g., $\\lambda = 1$) can provide a good fit to the data.\n",
    "So we have to implememt an automated method to select the $\\lambda$ parameter. Concretely, we will use a cross validation set to evaluate how good each $\\lambda$ value is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the best $\\lambda$ value using the cross validation set, we can then evaluate the model on the test set to estimate how well the model will perform on actual unseen data.\n",
    "To achieve that, I implemented `validationCurve` which does this job. This function takes in input a list of values to try (*lambda_vec*) along with the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n",
    "train_error_lamb, validation_error_lamb, best_lamda = validationCurve(X_poly, y, X_poly_val, y_val, lambda_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(np.arange(lambda_vec.min(), lambda_vec.max()), train_error_lamb, label = 'Train')\n",
    "ax.plot(np.arange(lambda_vec.min(), lambda_vec.max()), validation_error_lamb,  label= 'Validation')#ax.annotate(\"Best Lambda: %.3f\" % best_lamda, xy=(2, 6), xytext=(0.001, 19),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.005))\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Lamda')\n",
    "plt.xlim(0,9)\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $\\lambda$ to choose is that value of $\\lambda$ that causes the least error on the validation set. In this case is the third value (in the plot is shown that is the second value, but it is actually the third since the index starts from zero) from our *lambda_vec*, which is 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.58127919],\n",
       "       [ 5.98824572],\n",
       "       [ 4.39488179],\n",
       "       [ 6.8846358 ],\n",
       "       [23.61431369],\n",
       "       [50.74367746],\n",
       "       [68.45252854],\n",
       "       [78.07489413],\n",
       "       [81.48716129],\n",
       "       [82.77865759]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_error_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the least error value for the validation set is the third (4.17235022)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing test set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tuned our $\\lambda$ parameter we can run our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error at the optimal lambda = 0.003 is equal to 3.14\n"
     ]
    }
   ],
   "source": [
    "initial_theta = np.ones((X_poly.shape[1], 1))\n",
    "lamb = 0.003\n",
    "alpha = 0.001\n",
    "iterations = 10000\n",
    "\n",
    "theta_optimal, J_history_poly = gradientDescent(X_poly, y, initial_theta, alpha, iterations, lamb)\n",
    "\n",
    "test_error = costFunctionReg(theta_optimal, X_poly_test, y_test, 0)\n",
    "print(\"The test error at the optimal lambda = 0.003 is equal to %.2f\" % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "ax.plot(X_test, y_test, 'x', color='r')\n",
    "X_fit, X_poly_fit = plotFit(min(X), max(X), mean, sigma, p)\n",
    "ax.plot(X_fit, np.dot(X_poly_fit, theta_optimal), '--', color='blue')\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the polynomial fit follows the data trend well! What abou the learning curve? Let's plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train_poly, error_val_poly = learningCurve(X_poly, y, X_poly_val, y_val, lamb)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(range(X_poly.shape[0]), error_train_poly, label = 'Training set error')\n",
    "ax.plot(range(X_poly.shape[0]), error_val_poly, label = 'Validation set error')\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Training samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that both the cross validation and trainig set error converge to a relatively low value and that there is no evident gab between them. \n",
    "\n",
    "So we managed to create a model with no bias and no variance! Great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
