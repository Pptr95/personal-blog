{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical training and testing for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that you completed your project and your network has $x$ % as accuracy on the test set. Is there a way to improve upon it? <br>\n",
    "In this blog post I am going to give some practical training and testing tips that you should try to **boost your network performance**. <br>\n",
    "Beaware that these tips should be applied **only after you have completed your project**, not during. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "Learning rate is a key hyperparameter. What is the **problem** with it? The problem is that is hard to find a perfecet learning rate.\n",
    "**Solution**: the solution consists in using a mixture of high and low learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate schedule: step\n",
    "The learning rate schedule introduces the idea of starting with a high lerning rate (like 0.1) and divide it by 10 whenever the the training error plateaus. <br>\n",
    "In practice the learning rate will be dynamic and not fixed and, over the training, it will look like this:\n",
    "<img src=\"img/step_decay.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is shown the effect of this dynamic learnig rate when a ResNet network is trained:\n",
    "<img src=\"img/lr_effect.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate schedule: warm-up\n",
    "All schedules we have defined starts with high learning rates. <<br>\n",
    "However, for very deep networks (eg. ResNet-110 on CIFAR-10) a high learning rate can slow down the convergence at the beginning of the training (eg. the accuracy remain at chance level for several epochs). **This is usually a symptom of poor initialization**: a way to counteract this is to **use a lower learning rate for a few epochs** until accuracy increases. <br>\n",
    "So the learning rate over the epochs will look something like this:\n",
    "<img src=\"img/lr_warmup.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate schedule: one cycle\n",
    "Another thing to try is the one cycle learning rate schedule. This type of schedule **modifies the learning rate after each mini-batch** (also referred as *itaration*), not after each epoch as the previous types of schedule. <br>\n",
    "Formally, if we training for a total number of iterations $l$, the learning rate for each iteration $i$ would be:\n",
    "<img src=\"img/onecycle.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
