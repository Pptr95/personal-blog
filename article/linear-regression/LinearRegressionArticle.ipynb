{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: from math concepts to implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression goal is to fit a straight line to the data, trying to capture what is the trend of the data. <br>\n",
    "In this article we will start with a quick intuitive walk-through of the mathematics behind this well-known problem and we will implement the whole algorithm from scratch. Of course there are already many libraries that implement linear regression, but our goal is to understand how linear regression works and I found that the best way to do that is to implement it from scratch.<br>\n",
    "In this article we will only explain how to fit a **straight line** to the data. In the next article we will explain also how to fit data that has not a linear  trend by creating non-linear model. So stay tuned!\n",
    "\n",
    "Let's look into some math aspetcs and then proceed to implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of linear regression is to minimize the cost function:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "where the hypothesis $h_\\theta$ is given by the linear model:\n",
    "\n",
    "$$ h_\\theta = \\theta^T x = \\theta_0 + \\theta_1 x_1 $$\n",
    "\n",
    "The model's parameters are the $\\theta_j$ values. These are the values that need to be adjusted to minimize cost $J(\\theta)$.\n",
    "One way to do this is to use the batch gradient descent algorithm. In batch gradient descent, each iteration performs the update:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$$ (simultaneously update $\\theta_j$ for all $j$);\n",
    "With each step of gradient descent, the parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will implement linear regression with one variable to predict profits for a food truck. Suppose you are a CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from cities. You would like to use this data to help you select which city to exapend to next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 17px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (18, 12)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 17px; }</style>\"))\n",
    "\n",
    "data = np.loadtxt('ex1data1.txt', delimiter=',')\n",
    "X = data[:, 0]\n",
    "y = data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize it since it has only two properties to plot (profit and population). Nevertheless, many other problems that you will encounter in real life are multi-dimensional and therefore can't be plotted on a 2-d plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "#fig = plt.figure()\n",
    "# adjust figure's size\n",
    "#ax = fig.add_subplot(1, 1, 1) \n",
    "# plot X data and the relative labels y\n",
    "#ax.plot(X, y, 'x', markersize=15, linewidth=7)\n",
    "#ax.scatter(X, y, s=500, marker='x',color='r',linewidths=8)\n",
    "# set label y\n",
    "#plt.ylabel('Profit in $10Ks')\n",
    "# set label x \n",
    "#plt.xlabel('Population of City in 10Ks')\n",
    "# show the figure\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/linear-regression/img/1.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the cost $J(\\theta)$ and Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X, y, theta):\n",
    "    return (1/(2*y.size))*sum((np.dot(theta.T, X.T).T-y)**2)\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, iterations):\n",
    "    m = y.shape[0]\n",
    "    J_history = np.zeros((iterations, 1))\n",
    "    for i in range(iterations):\n",
    "        hypothesis_minus_y = (np.matmul(theta.T, X.T).T-y).T\n",
    "        theta = (theta.T - (alpha/m)*(np.matmul(hypothesis_minus_y, X))).T\n",
    "        J_history[i] = computeCost(X, y, theta)\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y need to be shaped from (97,) to (97, 1)\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "# it is not necessarily to reshape X since we add the intercept term and X become already right shaped into (97, 2)\n",
    "#X = X.reshape(X.shape[0], 1)\n",
    "X = np.c_[np.ones(X.shape[0]), X] # adding column of ones to X to account for theta_0 (the intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: intercept=[-3.63029144], slope=[1.16636235]\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([[0, 0]]).T\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "theta, J_history = gradientDescent(X, y, theta, alpha, iterations)\n",
    "print(\"Theta found by gradient descent: intercept={0}, slope={1}\".format(theta[0],theta[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you performed gradient descent to learn minimize the cost function $J(\\theta)$, it is helpful to monitor the convergence by computing the cost. You should see that the cost $J(\\theta)$ is not-increasing at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "#ax.plot(range(iterations), J_history, linewidth=10, c='r')\n",
    "#plt.ylabel('Cost Value')\n",
    "#plt.xlabel('Iteration of Gradient Descent')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/linear-regression/img/2.png\" alt=\"\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(1, 1, 1) \n",
    "# plot X data and the relative labels y\n",
    "#ax.plot(X[:, 1], y[:, 0], 'x', label='Training Data')\n",
    "#ax.scatter(X[:, 1], y[:, 0], s=500, marker='x',color='r',linewidths=8, label='Training data')\n",
    "# plot the hypotesis line h = theta0*X[:, 0] + theta1*X[:, 1] \n",
    "#ax.plot(X[:,1], np.matmul(X, theta), linestyle='-', label='Linear Regression', linewidth=8, c='b')\n",
    "# set the legend for the labels: 'Training Data' and 'Linear Regression'\n",
    "#legend = ax.legend(loc='upper center', shadow=True)\n",
    "#plt.ylabel('Profit in $10Ks')\n",
    "#plt.xlabel('Population of City in 10Ks')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/linear-regression/img/3.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a population of 35000, we predict a profict of:  4520.0 $\n"
     ]
    }
   ],
   "source": [
    "predict1 = np.matmul(np.array([[1, 3.5]]), theta)\n",
    "print(\"For a population of 35000, we predict a profict of: \", round(predict1[0][0] * 10000), \"$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a population of 70000, we predict a profict of:  45342.0 $\n"
     ]
    }
   ],
   "source": [
    "predict2 = np.matmul(np.array([[1, 7]]), theta)\n",
    "print(\"For a population of 70000, we predict a profict of: \", round(predict2[0][0] * 10000), \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with multiple variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent sold and make a model of housing prices.\n",
    "We will consider now a dataset containing a training set of housing prices in Portland, Oregon.\n",
    "The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
    "X = data[:, 0:2]\n",
    "y = data[:, 2]\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(2, 1, 1) \n",
    "#ax.scatter(X[:, 0], y, s=500, marker='x',color='r',linewidths=8)\n",
    "#plt.ylabel('House prices')\n",
    "#plt.xlabel('Size of the house (square feet)')\n",
    "#plt.show()\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(2, 1, 1) \n",
    "#ax.scatter(X[:, 1], y, s=500, marker='x',color='r',linewidths=8)\n",
    "#plt.ylabel('House prices')\n",
    "#plt.xlabel('Number of bedrooms')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/linear-regression/img/4.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the values, note that house size are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient decent converge much more quikly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    average = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - average)/sigma\n",
    "    return X_norm, average, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, average, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: intercept=[340412.56301439], slope1=[109370.05670466], slope2=[-6500.61509507] \n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "interations = 1500\n",
    "# adding the intercept\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "theta, J_history = gradientDescent(X, y, theta, alpha,  interations)\n",
    "print(\"Theta found by gradient descent: intercept={0}, slope1={1}, slope2={2} \".format(theta[0],theta[1],theta[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the convergence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(1, 1, 1)\n",
    "#ax.plot(range(iterations), J_history, linewidth=10)\n",
    "#plt.ylabel('Cost Value')\n",
    "#plt.xlabel('Iterations of Gradient Descent')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/linear-regression/img/5.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction\n",
    "Estimate the price of a 1650 sq-ft, 3 bedrooms house:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.array([[1650, 3]])\n",
    "# first need to normalize our data\n",
    "new_data = (new_data - average) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the intercept\n",
    "new_data = np.c_[np.array([1]), new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated price for a house of 1650 sq-ft and 3 bedrooms:  293098.4666757651 $\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimated price for a house of 1650 sq-ft and 3 bedrooms: \", np.dot(new_data, theta)[0][0], \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the same optimized parameters found with Gradient Descent can be calculated in an elegant, efficient and closed form, using linear algebra. Specifically:\n",
    "$$\\theta = (X^T X)^{-1} X^Ty$$\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"loop until convergence\" like in gradient descent.\n",
    "This equation is preferred that Gradient Descent when the dataset is < 10000, otherwise Gradient Descent is preferred since it will converge faster. Furthermore, if the $X^T X$ does not admit inverse, this method cannot be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEquation(X, y):\n",
    "    XTX_inv= np.linalg.inv(np.matmul(X.T, X))\n",
    "    XTy = np.matmul(X.T, y)\n",
    "    theta = np.matmul(XTX_inv, XTy)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
    "X = data[:, 0:2]\n",
    "y = data[:, 2]\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the intercept\n",
    "X = np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta found by gradient descent: intercept=[89597.9095428], slope1=[139.21067402], slope2=[-8738.01911233] \n"
     ]
    }
   ],
   "source": [
    "theta = normalEquation(X, y)\n",
    "print(\"Theta found by gradient descent: intercept={0}, slope1={1}, slope2={2} \".format(theta[0],theta[1],theta[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated price for a house of 1650 sq-ft and 3 bedrooms:  293081.4643348931 $\n"
     ]
    }
   ],
   "source": [
    "new = np.array([[1, 1650, 3]])\n",
    "print(\"Estimated price for a house of 1650 sq-ft and 3 bedrooms: \", np.dot(new, theta)[0][0], \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, we obtained a predicted value very similar to the value we got earlier using Gradient Descent, which is: 293098.4666757651 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare our implementation to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
    "X = data[:, 0:2]\n",
    "y = data[:, 2]\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the intercept term\n",
    "X = np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# train the model using the training set\n",
    "regr.fit(X, y)\n",
    "\n",
    "# make prediction\n",
    "y_pred_regr = regr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**theta** are the parameters we found before with Normal Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error between my implementation and sklearn implementation is:  9.454185273745276e-18\n"
     ]
    }
   ],
   "source": [
    "# compare sklearn with my implementation using mean_squared_error\n",
    "error = mean_squared_error(np.matmul(X, theta), y_pred_regr)\n",
    "print(\"Error between my implementation and sklearn implementation is: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the mean squared error is really small! Great!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
