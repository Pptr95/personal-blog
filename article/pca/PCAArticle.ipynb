{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis: \"What\", \"When\" and \"How\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following article I am going to explain *what is* PCA (Principal Component Analysis), *when* use it and *how*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a machine learning task. You are given a dataset with 500 variables and you want to predict something but 500 variables are too many to consider.\n",
    "If you have worked with a lot of variables before, you know this can present problems. <br>\n",
    "Do you understand the relationships between each variable? <br>\n",
    "Do you have so many variables that you are in danger of overfitting your model to your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask the question, “How do I take all of the variables I have collected and focus on only a few of them?” In technical terms, you want to “reduce the dimension of your feature space.” By reducing the dimension of your feature space, you have fewer relationships between variables to consider and you are less likely to overfit your model. (Note: This does not immediately mean that overfitting, etc. are no longer concerns — but we are moving in the right direction!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat unsurprisingly, reducing the dimension of the feature space is called **dimensionality reduction**. There are many ways to achieve dimensionality reduction, but most of these techniques fall into one of two classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Elimination\n",
    "* Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature elimination** is what it sounds like: we reduce the feature space by eliminating features. Advantages of feature elimination methods include simplicity and maintaining interpretability of your variables.\n",
    "As a disadvantage, though, you gain no information from those variables you have dropped. By eliminating features, we have also entirely eliminated any benefits those dropped variables would bring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we would like, somehow, to retain the benefits of the variables we dropped. Here is where **Feature extraction** comes to aid. <br>\n",
    "Say we have ten independent variables. In feature extraction, we create ten “new” independent variables, where each “new” independent variable is a combination of each of the ten “old” independent variables. Doing so, we are still keeping the most valuable parts of our old variables, even when we drop one or more of these “new” variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA **are all independent of one another**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should I use PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Do you want to reduce the number of variables, but are not able to identify variables to completely remove from consideration?\n",
    "2. Do you want to ensure your variables are independent of one another?\n",
    "3. Are you comfortable making your independent variables less interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you answered “yes” to all three questions, then PCA is a good method to use. If you answered “no” to question 3, you **should not** use PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does PCA work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I am going to show how PCA works providing all the required functions I implemented to run the algorithm along with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 17px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import random\n",
    "from scipy import ndimage\n",
    "from skimage import io\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 17px; }</style>\"))\n",
    "\n",
    "def pca(X):\n",
    "    m = X.shape[0]\n",
    "    sigma = (1/m)*np.dot(X.T, X)\n",
    "    u, s, vh = np.linalg.svd(sigma, full_matrices=True, compute_uv=True)\n",
    "    return u, s\n",
    "\n",
    "def projectData(X, U, K):\n",
    "    U_reduced = U[:, 0:K]\n",
    "    return np.dot(X, U_reduced), U_reduced\n",
    "\n",
    "def recoverData(Z, U, K):\n",
    "    return np.dot(Z, U.T)\n",
    "\n",
    "def featureNormalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis= 0)\n",
    "    X = (X-mu)/sigma\n",
    "    return X, mu, sigma\n",
    "\n",
    "def drawLine(p1, p2, c):\n",
    "    plt.plot(p1, p2, color=c, linestyle='dashed')\n",
    "\n",
    "def displayData(X):\n",
    "    m = int(np.sqrt(X.shape[1]))\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.title('100 Random faces from the data set')\n",
    "    random.seed(0)\n",
    "    indexes = random.sample(range(1, X.shape[0]), 100)\n",
    "    images = X[indexes,:]\n",
    "    for i in np.arange(10):\n",
    "        for j in np.arange(10):\n",
    "            ax = fig.add_subplot(10, 10, (10*j+i)+1)\n",
    "            ax.matshow(ndimage.rotate(images[10*j+i,:].reshape(m,m),-90), cmap = 'gray')\n",
    "            plt.yticks(())\n",
    "            plt.xticks(())\n",
    "            plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show visually how PCA works, I a going to consider a 2D dataset and reducing it to one dimension (thus a straight line). The 2D dataset we take into account is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex7data1.mat')\n",
    "X = data['X']\n",
    "#plt.scatter(X[:, 0], X[:, 1], s=8, c='blue')\n",
    "#plt.axis('tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/1.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using PCA, it is important first to normalize the data by subtracting the mean value of each feature from the dataset, and scaling each dimension so that they are in the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, average, sigma = featureNormalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply PCA algorithm which is accomplished by the following steps:. <br>\n",
    "1. the first thing to do is to calculate a matrix that summarizes how our variables all relate to one another. This matrix is called *covariance matrix* and is given by:\n",
    "\n",
    "    $$\\sigma = \\frac{1}{m} X^TX$$\n",
    "\n",
    "    where *X* is the $m$ x $n$ data matrix (where $m$ is the number of rows and $n$ is the number of variables). <br><br>\n",
    "\n",
    "2. then, we run the *Singular Value Decomposition* (SVD) algorithm. SVD will return two things:\n",
    "    * the **direction vectors** of the data, namely the principal components (eigenvectors)\n",
    "    * the **magnitude** of the direction vectors (eigenvalues)<br>\n",
    "    With these information we can then understand the *directions* of our data and its *magnitude* (or how “important” each direction is)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two steps are accomplished by the `pca` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_vectors, magnitude = pca(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following code I am going to show the direction vectors and magnitude for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = average + 1.5 * magnitude[0] * direction_vectors[:, 0].T\n",
    "p2 = average + 1.5 * magnitude[1] * direction_vectors[:, 1].T\n",
    "#plt.scatter(X[:, 0], X[:, 1], s=8, c='blue')\n",
    "#plt.xlim(1, 7)\n",
    "#plt.ylim(3, 8)\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "#plt.draw()\n",
    "#drawLine(np.array([average[0], p1[0]]), np.array([average[1], p1[1]]), c='red')\n",
    "#drawLine(np.array([average[0], p2[0]]), np.array([average[1], p2[1]]), c='black')\n",
    "#plt.xlabel('1° principal component', fontsize=14, color='red');\n",
    "#plt.ylabel('2° principal component', fontsize=14, color='black');\n",
    "#plt.suptitle('original dataset with principal components', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/2.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “red direction” and the “black direction\" are the two main directions of our data. In this case, the “red direction” is the more important one. Given how the dots are arranged, can you see why the “red direction” looks more important than the “black direction?” (Hint: what would fitting a line of best fit to this data look like?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. then, we transform our original data to align with these directions (which are combinations of our original variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=2\n",
    "Z, _ = projectData(X_norm, direction_vectors, K)\n",
    "Z_norm, average, sigma = featureNormalize(Z)\n",
    "direction_vectors_2, magnitude = pca(Z)\n",
    "#p1 = average + 1.5 * magnitude[0] * direction_vectors_2[:, 0].T\n",
    "#p2 = average + 1.5 * magnitude[1] * direction_vectors_2[:, 1].T\n",
    "#plt.scatter(Z[:, 0], Z[:, 1], s=8, c='blue')\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "#plt.draw()\n",
    "#drawLine(np.array([average[0], p1[0]]), np.array([average[1], p1[1]]), c='red')\n",
    "#drawLine(np.array([average[0], p2[0]]), np.array([average[1], p2[1]]), c='black')\n",
    "#plt.suptitle('output from PCA', fontsize=14)\n",
    "#plt.xlabel('1° principal component', fontsize=14, color='red');\n",
    "#plt.ylabel('2° principal component', fontsize=14, color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/3.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note two things in this graphic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This charts show the exact same data as the previous chart, but this graph reflects the original data transformed so that our axes are now the principal components.\n",
    "* In both graphs, the principal components are perpendicular to one another. In fact, every principal component will **always** be orthogonal (a.k.a. official math term for perpendicular) to every other principal component. **So, because our principal components are orthogonal to one another, they are statistically linearly independent of one another**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting the data onto a lower dimension K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the principal components, we can use them to reduce the feature dimension of our dataset by projecting each example onto a lower dimensional space, $x^{(i)} \\rightarrow z^{(i)} $ (in this example, projecting the data from 2D to 1D). <br>\n",
    "To do that, we will use our direction vectors returned by PCA and project the example dataset into a K-dimensional space. K in this example is 1, so we will select the first column (principal component) from our *direction_vectors* (in the underneath formula and the following I indicate this matrix as **D**) matrix and then multiplying it to our normalized dataset.\n",
    "$$Z = D^T X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation is accomplished by the function `projectData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=1\n",
    "Z, direction_vectors_reduced = projectData(X_norm, direction_vectors, K)\n",
    "#plt.figure(figsize=(8, 3))\n",
    "#ones = np.ones((Z.shape[0], 1))\n",
    "#plt.scatter(Z, ones, s=8, c='blue')\n",
    "#plt.yticks(())\n",
    "#plt.axis('tight')\n",
    "#plt.suptitle('Projected the data onto a 1D', fontsize=14);\n",
    "#plt.xlabel('pc1', fontsize=14, color='red');\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/4.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting our data into a smaller space, we are reducing the dimensionality of our feature space… **but because we have transformed our data in these different “directions,” we have made sure to keep all original variables in our model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing an approximation of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After projecting the data onto a lower dimensional space, we can approximately recover the data by projecting them back onto the original high dimensional space. This action is accomplished by the funcion `recoverData`.\n",
    "\n",
    "$$Z = D^T X \\rightarrow X_{approx} = D Z^T \\approx X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstructed = recoverData(Z, direction_vectors_reduced, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], s=8, c='blue')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/5.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_norm.shape[0]):\n",
    "    p1 = X_norm[i, :]\n",
    "    p2 = X_reconstructed[i, :]\n",
    "    #drawLine(np.array([p1[0], p2[0]]), np.array([p1[1], p2[1]]), c='black')\n",
    "#plt.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], color='blue', s=8)\n",
    "#plt.scatter(X_norm[:, 0], X_norm[:, 1], s=30, color='red', facecolors='none')\n",
    "#plt.gca().set_aspect('equal', adjustable='box')\n",
    "#plt.draw()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/6.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the chart how the projections and then reconstruction affects the data. The original data points are indicated with the red circels, while the projected data points are indicated with the red dots. The projections effectively only retains the information in the direction given by \"red direction\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on the face dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to will apply PCA to a real life dataset. The idea here is that we have $32 \\times 32$ pixels greyscale pictures of faces and we want to reduce their size to $10 \\times 10$, keeping as much information as possible about the original image. We still want to be able to recognize whose face we are looking at. It turns out that this is prefectly possible running PCA on the picture and projecting it on the subspace mapped by its top 100 (we can choose as many as we wish) principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex7faces.mat')\n",
    "X = data['X']\n",
    "#displayData(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/7.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm, average, sigma = featureNormalize(X)\n",
    "direction_vectors, magnitude = pca(X_norm)\n",
    "K = 100\n",
    "Z, direction_vectors_reduced = projectData(X_norm, direction_vectors, K)\n",
    "X_reconstructed = recoverData(Z, direction_vectors_reduced, K)\n",
    "#displayData(X_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/pca/img/8.png\" alt=\"\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the reconstruction, it can be observed that the general and appearance of the face are kept while the fine details are lost. This is a remarkable reduction (more than 10$\\times$) in the dataset size that can help speed up the learning algorithms significantly. <br>\n",
    "For example, if someone were training a neural network to perform person recognitio (given a face image, predict the identity of the person), you can use the dimension reduced input of only a 100 dimensions instead of the original pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: how to estimate if we have chosen a good K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate if our choice of K is good, we need to take the magnitude vector ($M$ in the folowing formula) returned from *SVD* algorithm and verify that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\sum_{i=1}^{k} M_i}{\\sum_{i=1}^{m} M_i} \\geq 0.99 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into account the example of second the part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex7faces.mat')\n",
    "X = data['X']\n",
    "X_norm, average, sigma = featureNormalize(X)\n",
    "direction_vectors, magnitude = pca(X_norm)\n",
    "K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with K=100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.932"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(magnitude[:K].sum() / magnitude[:].sum(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that K=100 is not good enough, which means that we have lost to many informations during the dimensionality reduction. Try with K=350:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 350\n",
    "round(magnitude[:K].sum() / magnitude[:].sum(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "350 is a good value since it respects the disequation. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is essentially a method that reduces the dimension of the feature space in such a way that new variables are orthogonal to each other (i.e. they are independent or not correlated) and keep the most valuable parts of the variables we drop. The drawback is that it makes your independent variables less interpretable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
