{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *K*-means: the first approach to unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we are going to leave behind the world of supervised learning and dive into the fascinating (and much more complex!) unsupervised domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name suggests, a problem is unsupervised when our dataset is not labeled. Thus we cannot learn from a pre-existing outcome as the data points we have in hand are not directly linked to any specific predictable attribute. So, to be clearer, a typical supervised problem would come in the form of a matrix $\\textbf{X} \\in \\mathcal{R}^{N \\times d}$ (*N* data points, *d* features) and a dependent variable $\\textbf{y} \\in \\mathcal{R}^{N \\times 1}$ containing the piece of information we want our model to learn. On the contrary, in the unsupervised domain we are provided just with $\\textbf{X}$. Nothing more.\n",
    "\n",
    "What would we want to learn from data inputs which are not differentiable in the first place?\n",
    "\n",
    "Actually, the whole point is exactly in this question. As the inputs are not labeled and hence not differentiable we may want to differentiate them. The idea is therefore to find patterns in the data with the aim to split it into \"groups of interest\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *K*-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many available approaches to perform clustering. We are going to keep it simple and touch base on maybe the most well known strategy: *K-means clustering*. \n",
    "\n",
    "**K-means**  is a very intuitive way of splitting a data set into *K* distinct, non-overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the K-means algorithm step by step:\n",
    "\n",
    "1. Randomly select K observations. These will be the centers (centroids) of our first clusters.\n",
    "2. Calculate the data points closest (minimal square of Euclidean distance) to the *K* centroids defined above and assign each of them to one cluster. \n",
    "3. For every single cluster, calculate a new centroid, averaging the position of all the points assigned to the cluster itself.\n",
    "4. Iterate over 2 and 3 until the centroids don't move anymore which means that each observation has been assigned to the cluster within which the variation is minimal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we are going to implement the algorithm **from scratch** and afterwards we will apply it to some dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 17px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import random\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 17px; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the functions that implement the algorithm described earlier along with some util functions for plotting the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestCentroids(X, centroids):\n",
    "    idx = np.zeros((X.shape[0], 1))\n",
    "    for p in range(X.shape[0]):\n",
    "        idx[p] = np.argmin(np.linalg.norm(X[p, :] - centroids, axis=1))\n",
    "    return idx + 1\n",
    "\n",
    "def computeCentroids(X, idx, K): \n",
    "    centroids = np.zeros((K, X.shape[1]))\n",
    "    for i in range(K):\n",
    "        centroids[i, :] = np.mean(X[(idx == (i+1)).nonzero()[0], :], axis=0)\n",
    "    return centroids\n",
    "\n",
    "def kMeansInitCentroids(X, K):\n",
    "    X = np.random.permutation(X)\n",
    "    return X[:K, :]\n",
    "\n",
    "def plotDataPoints(X, idx, K):\n",
    "    colors = cm.rainbow(np.linspace(0, 1, 100))\n",
    "    for k in range(1, K + 1):\n",
    "        plt.scatter(X[(idx == (k)).nonzero()[0], 0], X[(idx == (k)).nonzero()[0], 1], s = 5, color=colors[random.randint(0, 99)])\n",
    "    \n",
    "def drawLine(p1, p2):\n",
    "    plt.plot(p1, p2, 'ro--')\n",
    "       \n",
    "def runkMeans(X, initial_centroids, max_iters, plot_clusters_and_centroids = False):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    idx = np.zeros((m, 1))\n",
    "    for i in range(max_iters):\n",
    "        idx = findClosestCentroids(X, centroids)       \n",
    "        centroids = computeCentroids(X, idx, K)\n",
    "        if plot_clusters_and_centroids == True:\n",
    "            plotDataPoints(X, idx, K)\n",
    "    if plot_clusters_and_centroids == True:\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], linewidths=3, s = 40, c = 'black', marker='x')\n",
    "    return centroids, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means in action on a 2D dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing our functions, we are going to use a 2D dataset since we can easily see how the algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex7data2.mat')\n",
    "X = data['X']\n",
    "K = 3\n",
    "max_iters = 10\n",
    "initial_centroids = kMeansInitCentroids(X, K)\n",
    "#centroids, idx = runkMeans(X, initial_centroids, max_iters, plot_clusters_and_centroids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/k-means/img/1.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the implementation of our algorithm performs pretty well on this dataset. <br>\n",
    "In this case, it can be clearly seen that there are three different clusters. This is why I have selected upfront the number of clusters I wanted to group. <br>\n",
    "But what if my dataset has a number of features (thus, the dimension of the dataset) $>4$? In that case I cannot plot the dataset and therefore I cannot choose upfront the number of clusters. There are different methods to counter this problem and we going to explain one in a moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression with *K*-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we cann apply K-means to **image compression**. In a straightforward 24-bit color representation of an image, 1 each pixel is represented as three 8-bit unsigned integers (ranging from 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding. Our image contains thousands of colors, and in this part of the exercise, we will reduce the number of colors to 16 colors.\n",
    "\n",
    "By making this reduction, it is possible to represent (compress) the photo in an efficient way. Specifically, we only need to store the RGB values of the 16 selected colors, and for each pixel in the image we now need to only store the index of the color at that location (where only 4 bits are necessary to represent 16 possibilities).\n",
    "\n",
    "We will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image. Concretely, we will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group (cluster) the pixels in the 3-dimensional RGB space. Once we have computed the cluster centroids on the image, we will then use the 16 colors to replace the pixels in the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the picture we are going to compress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "bird = io.imread('bird_small.png')\n",
    "\n",
    "#plt.imshow(bird)\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/k-means/img/2.png\" alt=\"\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide by 255 so that all values are in the range 0 - 1\n",
    "bird = bird / 255\n",
    "img_size = bird.shape\n",
    "\n",
    "# Reshape the image into an Nx3 matrix where N = number of pixels.\n",
    "# Each row will contain the Red, Green and Blue pixel values\n",
    "# This gives us our dataset matrix X that we will use K-Means on\n",
    "X = bird.reshape((img_size[0] * img_size[1], img_size[2]))\n",
    "K = 16\n",
    "max_iters = 12\n",
    "\n",
    "# When using K-Means, it is important the initialize the centroids randomly\n",
    "initial_centroids = kMeansInitCentroids(X, K)\n",
    "\n",
    "# running K-means\n",
    "centroids, idx = runkMeans(X, initial_centroids, max_iters,plot_clusters_and_centroids=False)\n",
    "\n",
    "bird_compressed = np.zeros((X.shape[0], X.shape[1]))\n",
    "\n",
    "# We can now recover the image from the indices (idx) by mapping each pixel\n",
    "# (specified by it's index in idx) to the centroid value\n",
    "for i, cluster in enumerate(idx):\n",
    "    bird_compressed[i,:] = centroids[int(cluster[0]) - 1,:]\n",
    "\n",
    "# Reshape the recovered image into proper dimensions\n",
    "bird_compressed = bird_compressed.reshape((img_size[0], img_size[1], img_size[2]))\n",
    "\n",
    "#fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "#ax1 = fig.add_subplot(121)\n",
    "#ax1.imshow(bird)\n",
    "#ax1.set_title('Original Image')\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "\n",
    "#ax2 = fig.add_subplot(122)\n",
    "#ax2.imshow(bird_compressed)\n",
    "#ax2.set_title('Compressed Image')\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/k-means/img/3.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the compressed image has a **much lower resolution** than the original one. <br>\n",
    "This is due to the fact that we are just using 16 colors to represent our exotic bird!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the good number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, a **drawback** of K-means is that we need to choose the number of clusters upfront, which is clearly not ideal. This means we need to have a deep understanding of the problem we want to solve before putting our hands on it, just to be able to formulate a logical hipothesys around an appropriate number of clusters. <br> We are going to explain a popular method that tries to solve this proble called **elbow method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow method give us an idea on what a good K number of clusters would be based on the sum of squared distance (SSE) between data points and their assigned clusters’ centroids. We pick K at the spot where SSE starts to flatten out and forming an elbow. We’ll use the previous dummy dataset and evaluate SSE for different values of K and see where the curve might form an elbow and flatten out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('ex7data2.mat')\n",
    "X = data['X']\n",
    "\n",
    "sse = []\n",
    "list_k = list(range(1, 10))\n",
    "for K in list_k:   \n",
    "    max_iters = 10\n",
    "    initial_centroids = kMeansInitCentroids(X, K)\n",
    "    centroids, idx = runkMeans(X, initial_centroids, max_iters, plot_clusters_and_centroids=False)\n",
    "\n",
    "    sse_sum = 0\n",
    "    for ki in range(1, K + 1):\n",
    "        sse_sum += sum(np.linalg.norm(X[(idx == (ki)).nonzero()[0], :] - centroids[ki - 1], axis=1))\n",
    "    sse.append(sse_sum)\n",
    "# Plot sse against k\n",
    "#plt.figure(figsize=(6, 6));\n",
    "#plt.plot(list_k, sse, '-o');\n",
    "#plt.xlabel(r'Number of clusters K');\n",
    "#plt.ylabel('Sum of squared distance');\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/k-means/img/4.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows that K=3 is not a bad choice since SSE starts to flatten out and forming an elbow from that point onwards. <br>\n",
    "Sometimes it’s still hard to figure out a good number of clusters to use because the curve is monotonically decreasing and may not show any elbow or has an obvious point where the curve starts flattening out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to sklearn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test whether our implementation is correct, I usually compare my implementation to **sklearn implementation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "data = sio.loadmat('ex7data2.mat')\n",
    "X = data['X']\n",
    "K = 3\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotDataPoints(X, kmeans.labels_ + 1, K)\n",
    "#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], linewidths=3, s = 40, c = 'black', marker='x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/k-means/img/5.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that sklearn clusters the data points **exactly** as my implementation did. I am over the moon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
